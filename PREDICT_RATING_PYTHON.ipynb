{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING PANDAS AND NUMPY FOR DATA MANIPULATION AND NUMERICAL COMPUTATIONS  \n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE TOURISM DATASET FROM A CSV FILE INTO A DATAFRAME  \n",
    "Tourism_df = pd.read_csv(r\"D:\\1final ds\\final_ds2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAYING THE FIRST FIVE ROWS OF THE TOURISM DATAFRAME  \n",
    "Tourism_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING FOR MISSING VALUES IN EACH COLUMN OF THE TOURISM DATAFRAME  \n",
    "Tourism_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COUNTING THE NUMBER OF DUPLICATE ROWS IN THE TOURISM DATAFRAME  \n",
    "Tourism_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING REGRESSION MODELS (XGBOOST, DECISION TREE, RANDOM FOREST) AND RANDOMIZED SEARCH FOR HYPERPARAMETER TUNING  \n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING FUNCTIONS FOR DATA SPLITTING, MODEL EVALUATION METRICS (MAE, MSE), AND OTHER PERFORMANCE MEASURES  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALLING THE CATEGORY_ENCODERS LIBRARY FOR ENCODING CATEGORICAL VARIABLES  \n",
    "pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING TARGET ENCODER FOR HANDLING CATEGORICAL VARIABLES, ONE-HOT ENCODER FOR CREATING DUMMY VARIABLES,  \n",
    "# AND STANDARD SCALER FOR FEATURE SCALING  \n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECTING RELEVANT FEATURES FOR MODEL TRAINING, INCLUDING VISIT DETAILS, ATTRACTION INFORMATION, AND LOCATION DATA  \n",
    "selected_features = [\n",
    "    \"VisitYear\",        \n",
    "    \"VisitMonth\",       \n",
    "    \"VisitModeName\",    \n",
    "    \"AttractionId\",     \n",
    "    \"Attraction\",      \n",
    "    \"AttractionType\",  \n",
    "    \"CountryId\",        \n",
    "    \"RegionId\"          \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ENCODING CATEGORICAL FEATURES USING ONE-HOT ENCODING AND TARGET ENCODING  \n",
    "# CONVERTING BOOLEAN COLUMNS TO INTEGER TYPE FOR COMPATIBILITY  \n",
    "# DROPPING ORIGINAL CATEGORICAL COLUMNS AFTER ENCODING AND CONCATENATING ENCODED FEATURES  \n",
    "categorical_features = [\"VisitModeName\", \"AttractionType\"]\n",
    "\n",
    "df_selected = Tourism_df[selected_features + [\"Rating\"]].copy()\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_features = ohe.fit_transform(df_selected[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(categorical_features))\n",
    "\n",
    "bool_cols = df_selected.select_dtypes(include=[\"bool\"]).columns\n",
    "df_selected[bool_cols] = df_selected[bool_cols].astype(int)\n",
    "\n",
    "target_enc = TargetEncoder()\n",
    "df_selected[\"Attraction\"] = target_enc.fit_transform(df_selected[\"Attraction\"], df_selected[\"Rating\"])\n",
    "\n",
    "df_selected = df_selected.drop(columns=categorical_features)\n",
    "df_selected = pd.concat([df_selected, encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATING FEATURES (X) AND TARGET VARIABLE (Y) FOR MODEL TRAINING  \n",
    "X = df_selected.drop(columns=[\"Rating\"])\n",
    "y = df_selected[\"Rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAYING THE FIRST FIVE ROWS OF THE FEATURE MATRIX (X)  \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLYING STANDARD SCALER TO NORMALIZE THE FEATURE MATRIX (X) FOR BETTER MODEL PERFORMANCE  \n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING THE DATA INTO TRAINING (80%) AND TESTING (20%) SETS WITH A FIXED RANDOM STATE FOR REPRODUCIBILITY  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING A DECISION TREE REGRESSOR WITH A MAX DEPTH OF 5 AND MAKING PREDICTIONS ON THE TEST SET  \n",
    "dt_model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor → MAE: 0.6875118080483658, RMSE: 0.9525885264963009\n"
     ]
    }
   ],
   "source": [
    "# ROUNDING PREDICTIONS TO THE NEAREST INTEGER AND CLIPPING VALUES TO ENSURE THEY FALL WITHIN THE VALID RATING RANGE (1 TO 5)  \n",
    "# CALCULATING MEAN ABSOLUTE ERROR (MAE) AND ROOT MEAN SQUARED ERROR (RMSE) TO EVALUATE MODEL PERFORMANCE  \n",
    "dt_pred = np.round(dt_pred).astype(int)\n",
    "dt_pred = np.clip(dt_pred, 1, 5) \n",
    "\n",
    "dt_mae = mean_absolute_error(y_test, dt_pred)\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_test, dt_pred))\n",
    "\n",
    "print(f\"Decision Tree Regressor → MAE: {dt_mae}, RMSE: {dt_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING A RANDOM FOREST REGRESSOR WITH 50 TREES AND A MAX DEPTH OF 5 FOR PREDICTIVE MODELING  \n",
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUNDING AND CLIPPING RANDOM FOREST PREDICTIONS TO ENSURE THEY FALL WITHIN THE VALID RATING RANGE (1 TO 5)  \n",
    "# CALCULATING MAE AND RMSE TO EVALUATE THE PERFORMANCE OF THE RANDOM FOREST REGRESSOR  \n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_pred = np.round(rf_pred).astype(int)\n",
    "rf_pred = np.clip(rf_pred, 1, 5) \n",
    "\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "\n",
    "print(f\"Random Forest Regressor → MAE: {rf_mae}, RMSE: {rf_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMING RANDOMIZED SEARCH CROSS-VALIDATION TO FIND THE BEST HYPERPARAMETERS FOR THE RANDOM FOREST REGRESSOR  \n",
    "# SEARCHING OVER DIFFERENT VALUES FOR NUMBER OF ESTIMATORS, MAX DEPTH, MIN SAMPLES SPLIT, MIN SAMPLES LEAF, AND MAX FEATURES  \n",
    "# SELECTING THE BEST MODEL BASED ON MINIMIZING MEAN ABSOLUTE ERROR (MAE)  \n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_random = RandomizedSearchCV(rf, param_dist, n_iter=20, cv=5, scoring='neg_mean_absolute_error', random_state=42, n_jobs=-1)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "best_rf = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor → MAE: 0.6868505573398829, RMSE: 0.9480657369624302\n"
     ]
    }
   ],
   "source": [
    "# EVALUATING THE BEST RANDOM FOREST MODEL WITH ROUNDED AND CLIPPED PREDICTIONS  \n",
    "rf_pred = best_rf.predict(X_test)\n",
    "rf_pred = np.round(rf_pred).astype(int)\n",
    "rf_pred = np.clip(rf_pred, 1, 5) \n",
    "\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "\n",
    "print(f\"Random Forest Regressor → MAE: {rf_mae}, RMSE: {rf_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AN XGBOOST REGRESSOR WITH 100 ESTIMATORS, LEARNING RATE OF 0.1, AND MAX DEPTH OF 6  \n",
    "model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING PREDICTIONS WITH XGBOOST, ROUNDING AND CLIPPING THEM TO VALID RATING RANGE (1 TO 5)  \n",
    "# CALCULATING MAE AND RMSE TO EVALUATE MODEL PERFORMANCE  \n",
    "y_pred_xgb = model.predict(X_test)\n",
    "\n",
    "dt_pred = np.round(y_pred_xgb).astype(int)\n",
    "dt_pred = np.clip(dt_pred, 1, 5) \n",
    "print(dt_pred)\n",
    "\n",
    "dt_mae = mean_absolute_error(y_test, dt_pred)\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_test, dt_pred))\n",
    "\n",
    "print(f\"Xgboost Regressor → MAE: {dt_mae}, RMSE: {dt_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMING RANDOMIZED SEARCH TO TUNE XGBOOST HYPERPARAMETERS AND SELECT THE BEST MODEL  \n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=param_grid, n_iter=20, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_xgb = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regressor (Tuned) → MAE: 0.709, RMSE: 0.904\n"
     ]
    }
   ],
   "source": [
    "# EVALUATING THE TUNED XGBOOST MODEL USING MAE AND RMSE  \n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"XGBoost Regressor (Tuned) → MAE: {mae:.3f}, RMSE: {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING JOBLIB FOR SAVING AND LOADING MODELS  \n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE TARGET ENCODER MODEL FOR FUTURE USE  \n",
    "joblib.dump(target_enc, r\"D:\\1final ds\\predrating_target.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE STANDARD SCALER MODEL FOR FUTURE USE  \n",
    "joblib.dump(scaler, r\"D:\\1final ds\\predscalar.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE ONE-HOT ENCODER MODEL FOR FUTURE USE  \n",
    "joblib.dump(ohe, r\"D:\\1final ds\\preddump.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE BEST XGBOOST MODEL FOR TOURISM RECOMMENDATION  \n",
    "joblib.dump(best_xgb, r\"D:\\1final ds\\predxgb.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
